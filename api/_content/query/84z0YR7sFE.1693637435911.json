[{"_path":"/topics/ai/cnn","_dir":"ai","_draft":false,"_partial":false,"_locale":"","_empty":false,"title":"Convolutional Neural Network","description":"How Convolution Neural Network works.","author":"Din Sokheng","slug":"ai-cnn","tag":"ai-cnn","img":"/topics/ai/figure-dnn.png","body":{"type":"root","children":[{"type":"element","tag":"h1","props":{"id":"convolutional-neural-network-cnn"},"children":[{"type":"text","value":"Convolutional Neural Network (CNN)"}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"Convolutional Neural Networks (CNNs) are a class of deep learning models primarily used for image and video analysis. They have revolutionized computer vision tasks such as image classification, object detection, and image segmentation. CNNs are designed to automatically learn hierarchical features from input data, which makes them particularly effective for tasks where the spatial relationships in data are essential, like recognizing patterns in images."}]},{"type":"element","tag":"h2","props":{"id":"forward-propagation"},"children":[{"type":"text","value":"Forward Propagation"}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"In the forward propagation phase of a CNN, the input image is convolved with learnable filters, followed by non-linear activation functions like ReLU (Rectified Linear Unit). The output of this convolutional layer is then pooled to reduce dimensionality and make the network translation-invariant. These operations are typically represented mathematically as:"}]},{"type":"element","tag":"ul","props":{},"children":[{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Convolution: "},{"type":"element","tag":"math-render","props":{"latex-express":" Z^{[l]} = (W^{[l]} * A^{[l-1]}) + b^{[l]}","styles":"inline-flex"},"children":[]}]},{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Activation: "},{"type":"element","tag":"math-render","props":{"latex-express":"A^{[l]} = \\text{ReLU}(Z^{[l]})","styles":"inline-flex"},"children":[]}]},{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Pooling: "},{"type":"element","tag":"math-render","props":{"latex-express":"A^{[l]} = \\text{MaxPooling}(A^{[l-1]})","styles":"inline-flex"},"children":[]}]}]},{"type":"element","tag":"h2","props":{"id":"backward-propagation"},"children":[{"type":"text","value":"Backward Propagation"}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"Backward propagation is the process of computing gradients with respect to the loss function, which allows the network to update its parameters during training. The gradients are computed using the chain rule of calculus and are used to adjust the convolutional filter weights and biases. The formulas for backward propagation are as follows:"}]},{"type":"element","tag":"ul","props":{},"children":[{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Convolutional Layer Backpropagation:"},{"type":"element","tag":"ul","props":{},"children":[{"type":"element","tag":"li","props":{},"children":[{"type":"element","tag":"math-render","props":{"latex-express":" \\frac{\\partial \\mathcal{L}}{\\partial W^{[l]}} = dZ^{[l]} * A^{[l-1]T}  ","styles":"inline-flex"},"children":[]}]},{"type":"element","tag":"li","props":{},"children":[{"type":"element","tag":"math-render","props":{"latex-express":" \\frac{\\partial \\mathcal{L}}{\\partial b^{[l]}} = \\sum dZ^{[l]} ","styles":"inline-flex"},"children":[]}]},{"type":"element","tag":"li","props":{},"children":[{"type":"element","tag":"math-render","props":{"latex-express":" \\frac{\\partial \\mathcal{L}}{\\partial A^{[l-1]}} = W^{[l]T} * dZ^{[l]} ","styles":"inline-flex"},"children":[]}]}]}]},{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Activation Layer Backpropagation (ReLU):"},{"type":"element","tag":"ul","props":{},"children":[{"type":"element","tag":"li","props":{},"children":[{"type":"element","tag":"math-render","props":{"latex-express":" \\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}} = dA^{[l]} \\odot \\text{ReLU}'(Z^{[l]})","styles":"inline-flex"},"children":[]}]}]}]},{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Pooling Layer Backpropagation:"},{"type":"element","tag":"ul","props":{},"children":[{"type":"element","tag":"li","props":{},"children":[{"type":"element","tag":"math-render","props":{"latex-express":" \\frac{\\partial \\mathcal{L}}{\\partial A^{[l-1]}} = \\text{upsample}(dA^{[l]})","styles":"inline-flex"},"children":[]}]}]}]}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"These gradients are then used to update the network's parameters via optimization algorithms like gradient descent, enabling the CNN to learn and improve its performance over time through the training process."}]}],"toc":{"title":"","searchDepth":2,"depth":2,"links":[{"id":"forward-propagation","depth":2,"text":"Forward Propagation"},{"id":"backward-propagation","depth":2,"text":"Backward Propagation"}]}},"_type":"markdown","_id":"content:topics:ai:cnn.md","_source":"content","_file":"topics/ai/cnn.md","_extension":"md"}]